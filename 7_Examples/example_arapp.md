#ARApp - iOS Example
ARApp is designed specifically for the [iOS][ios_about] environment, and so differs somewhat from the design of the examples for the desktop platforms. It respects the iOS's model-view-controller design pattern. Calls to ARToolKit's C functions (part of the "model") are made entirely by the ARViewController class. ARViewController is a subclass of UIViewController, and is designed to be able to be reused in your own applications. When instantiated, it opens a video window, and creates an OpenGL rendering context. When dismissed, it disposes of the OpenGL context and closes the video window.

As provided, the ARApp example includes a MainWindow NIB file which includes an instance of the ARViewController class. The application delegate and the ARViewController are connected in the NIB. You can easily modify this design, e.g. to load a different view controller when the application is opened. You could then instantiate ARViewController via a NIB or in code.

The OpenGL drawing code is contained within the ARView class and its superclass EAGLView. EAGLView extends UIView and implements either an OpenGL ES 1.1 or 2.0 rendering context. ARView extends the EAGLView class to provide important functionality for the AR environment, including compositing the OpenGL drawing together with the camera image.

The contents of the virtual environment are abstracted into the VirtualEnvironment class (instantiated by the view controller) and the VEObject class and its subclasses.

- VEObject: Root class representing behaviour of a single object in the virtual environment. Includes methods for updating the pose (position and orientation) of the object, setting a "local" pose for the object, testing and setting object visibility and other properties.
- VEObjectOBJ: A subclass which represents a drawable Wavefront .OBJ model file. It registers for drawing notifications from the ARView and draws the model (using the Eden glm code).
- VEObjectOSG: Similar to VEObjectOBJ, it allows models to be loaded and drawn using the OpenSceneGraph framework. Connects to OSG via the arOSG library. Supported file types are .osg and .ive. Models may include transform-based animation, particle effects, custom lighting, and more.
- VEObjectMovie: Loads and draws a movie file from the local file system as a video texture, using the MovieVideo class. It allows playback of MPEG4 video (with or without audio) in the virtual environment. Recommended maximum movie size is 512 pixels or less in both the vertical and horizontal dimensions.

The virtual environment connects to the OpenGL drawing and the ARToolKit tracking using NSNotifications generated by the ARView and ARMarker classes. These classes uses NSNotifications to tell the virtual environment when to update object poses with newly-processed marker data, and when to draw the updated objects.

The ARMarker class includes a class method to read marker definitions from a file (markers.dat) and to instantiate ARMarker instances. Full support for filtering of marker poses is included in the ARMarker class. Filtering helps remove unwanted high-frequency "jittering" or "oscillating" of marker poses when the marker pose is poorly conditioned (typically when the marker is a long distance and/or perpendicular to the camera line-of-sight). Filtering can be very easily enabled by adding a line "FILTER 1" to a marker definition in the markers.dat file.

If you have any further questions about the design, or how the app fits together, please ask on the community forum (rather than by email) so that others can benefit from the answers.

[ios_about]: 5_iOS:ios_about
